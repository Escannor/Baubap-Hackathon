{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, matthews_corrcoef, f1_score, brier_score_loss\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (2563, 548) y: (2563,)\n"
     ]
    }
   ],
   "source": [
    "#import pandas as pd\n",
    "df = pd.read_csv('../data/training.csv')\n",
    "\n",
    "X = df.drop('Target', axis=1)\n",
    "y = df['Target'].copy()\n",
    "\n",
    "print('X:', X.shape, 'y:', y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work with non-numeric features\n",
    "X = X.select_dtypes(exclude=['object'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Crear un objeto SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')  # Puedes cambiar 'mean' por 'median', 'most_frequent', o cualquier valor constante\n",
    "\n",
    "# Aplicar la imputaci√≥n a todas las columnas\n",
    "X = pd.DataFrame(imputer.fit_transform(X), columns=X.columns)\n",
    "\n",
    "# Imprimir el DataFrame resultante con valores imputados\n",
    "# print(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2563, 39)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove low variance features\n",
    "selection = VarianceThreshold(threshold=(0.1))\n",
    "X_transformed = selection.fit_transform(X)\n",
    "\n",
    "selected_columns = list(X.columns[selection.get_support()])\n",
    "# print(selected_columns)\n",
    "\n",
    "# X = selection.fit_transform(X)\n",
    "X = X_transformed; del(X_transformed)\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2050, 39), (513, 39))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data splitting\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, stratify=y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target\n",
      "0    1828\n",
      "1     222\n",
      "Name: count, dtype: int64 Target\n",
      "0    457\n",
      "1     56\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y_train.value_counts(), y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9073170731707317\n",
      "- MCC: 0.37531213176181616\n",
      "- F1 score: 0.8850438839083913\n",
      "- Brier score: 0.09268292682926829\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8732943469785575\n",
      "- MCC: 0.07488162711470178\n",
      "- F1 score: 0.8420409294240728\n",
      "- Brier score: 0.1267056530214425\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "knn = KNeighborsClassifier(3) # Define classifier\n",
    "knn.fit(X_train, y_train) # Train model\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = knn.predict(X_train)\n",
    "y_test_pred = knn.predict(X_test)\n",
    "\n",
    "# Training set performance\n",
    "knn_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "knn_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "knn_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "knn_train_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "# Test set performance\n",
    "knn_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "knn_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "knn_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "knn_test_brier = brier_score_loss(y_test, y_test_pred) # Calculate Brier Score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % knn_train_accuracy)\n",
    "print('- MCC: %s' % knn_train_mcc)\n",
    "print('- F1 score: %s' % knn_train_f1)\n",
    "print('- Brier score: %s' % knn_train_brier)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % knn_test_accuracy)\n",
    "print('- MCC: %s' % knn_test_mcc)\n",
    "print('- F1 score: %s' % knn_test_f1)\n",
    "print('- Brier score: %s' % knn_test_brier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       [1.        , 0.        ],\n",
       "       ...,\n",
       "       [1.        , 0.        ],\n",
       "       [0.66666667, 0.33333333],\n",
       "       [1.        , 0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.predict_proba(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machine (Radial basis function kernel)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9980487804878049\n",
      "- MCC: 0.9898676257556559\n",
      "- F1 score: 0.9980409775483746\n",
      "- Brier score: 0.001951219512195122\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8908382066276803\n",
      "- MCC: 0.0\n",
      "- F1 score: 0.8394083720182473\n",
      "- Brier score: 0.10916179337231968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "svm_rbf = SVC(gamma=2, C=1)\n",
    "svm_rbf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = svm_rbf.predict(X_train)\n",
    "y_test_pred = svm_rbf.predict(X_test)\n",
    "\n",
    "# Training set performance\n",
    "svm_rbf_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "svm_rbf_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "svm_rbf_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "svm_rbf_train_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "# Test set performance\n",
    "svm_rbf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "svm_rbf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "svm_rbf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "svm_rbf_test_brier = brier_score_loss(y_test, y_test_pred) # Calculate Brier Score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % svm_rbf_train_accuracy)\n",
    "print('- MCC: %s' % svm_rbf_train_mcc)\n",
    "print('- F1 score: %s' % svm_rbf_train_f1)\n",
    "print('- Brier score: %s' % svm_rbf_train_brier)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % svm_rbf_test_accuracy)\n",
    "print('- MCC: %s' % svm_rbf_test_mcc)\n",
    "print('- F1 score: %s' % svm_rbf_test_f1)\n",
    "print('- Brier score: %s' % svm_rbf_test_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9\n",
      "- MCC: 0.27538212309834736\n",
      "- F1 score: 0.8679479671122976\n",
      "- Brier score: 0.1\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8771929824561403\n",
      "- MCC: 0.0008353219102231928\n",
      "- F1 score: 0.8357966075037109\n",
      "- Brier score: 0.12280701754385964\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(max_depth=5) # Define classifier\n",
    "dt.fit(X_train, y_train) # Train model\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = dt.predict(X_train)\n",
    "y_test_pred = dt.predict(X_test)\n",
    "\n",
    "# Training set performance\n",
    "dt_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "dt_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "dt_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "dt_train_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "# Test set performance\n",
    "dt_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "dt_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "dt_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "dt_test_brier = brier_score_loss(y_test, y_test_pred) # Calculate Brier Score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % dt_train_accuracy)\n",
    "print('- MCC: %s' % dt_train_mcc)\n",
    "print('- F1 score: %s' % dt_train_f1)\n",
    "print('- Brier score: %s' % dt_train_brier)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % dt_test_accuracy)\n",
    "print('- MCC: %s' % dt_test_mcc)\n",
    "print('- F1 score: %s' % dt_test_f1)\n",
    "print('- Brier score: %s' % dt_test_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.9829268292682927\n",
      "- MCC: 0.9091299899324812\n",
      "- F1 score: 0.9822772630128498\n",
      "- Brier score: 0.01707317073170732\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8869395711500975\n",
      "- MCC: -0.02189981576173119\n",
      "- F1 score: 0.8374615372223028\n",
      "- Brier score: 0.11306042884990253\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(n_estimators=10) # Define classifier\n",
    "rf.fit(X_train, y_train) # Train model\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = rf.predict(X_train)\n",
    "y_test_pred = rf.predict(X_test)\n",
    "\n",
    "# Training set performance\n",
    "rf_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "rf_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "rf_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "rf_train_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "# Test set performance\n",
    "rf_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "rf_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "rf_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "rf_test_brier = brier_score_loss(y_test, y_test_pred) # Calculate Brier Score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % rf_train_accuracy)\n",
    "print('- MCC: %s' % rf_train_mcc)\n",
    "print('- F1 score: %s' % rf_train_f1)\n",
    "print('- Brier score: %s' % rf_train_brier)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % rf_test_accuracy)\n",
    "print('- MCC: %s' % rf_test_mcc)\n",
    "print('- F1 score: %s' % rf_test_f1)\n",
    "print('- Brier score: %s' % rf_test_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.8917073170731707\n",
      "- MCC: 0.0\n",
      "- F1 score: 0.8406606372407202\n",
      "- Brier score: 0.10829268292682927\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8908382066276803\n",
      "- MCC: 0.0\n",
      "- F1 score: 0.8394083720182473\n",
      "- Brier score: 0.10916179337231968\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(alpha=1, max_iter=1000)\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = mlp.predict(X_train)\n",
    "y_test_pred = mlp.predict(X_test)\n",
    "\n",
    "# Training set performance\n",
    "mlp_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "mlp_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "mlp_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "mlp_train_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "# Test set performance\n",
    "mlp_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "mlp_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "mlp_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "mlp_test_brier = brier_score_loss(y_test, y_test_pred) # Calculate Brier Score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % mlp_train_accuracy)\n",
    "print('- MCC: %s' % mlp_train_mcc)\n",
    "print('- F1 score: %s' % mlp_train_f1)\n",
    "print('- Brier score: %s' % mlp_train_brier)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % mlp_test_accuracy)\n",
    "print('- MCC: %s' % mlp_test_mcc)\n",
    "print('- F1 score: %s' % mlp_test_f1)\n",
    "print('- Brier score: %s' % mlp_test_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 1.0\n",
      "- MCC: 1.0\n",
      "- F1 score: 1.0\n",
      "- Brier score: 0.0\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8830409356725146\n",
      "- MCC: 0.05684405386725444\n",
      "- F1 score: 0.8420991842044472\n",
      "- Brier score: 0.11695906432748537\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgbc = XGBClassifier(n_estimators=100, random_state=42)\n",
    "xgbc.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = xgbc.predict(X_train)\n",
    "y_test_pred = xgbc.predict(X_test)\n",
    "\n",
    "# Training set performance\n",
    "xgbc_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "xgbc_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "xgbc_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "xgbc_train_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "# Test set performance\n",
    "xgbc_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "xgbc_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "xgbc_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "xgbc_test_brier = brier_score_loss(y_test, y_test_pred) # Calculate Brier Score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % xgbc_train_accuracy)\n",
    "print('- MCC: %s' % xgbc_train_mcc)\n",
    "print('- F1 score: %s' % xgbc_train_f1)\n",
    "print('- Brier score: %s' % xgbc_train_brier)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % xgbc_test_accuracy)\n",
    "print('- MCC: %s' % xgbc_test_mcc)\n",
    "print('- F1 score: %s' % xgbc_test_f1)\n",
    "print('- Brier score: %s' % xgbc_test_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Stacked model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model performance for Training set\n",
      "- Accuracy: 0.8917073170731707\n",
      "- MCC: 0.0\n",
      "- F1 score: 0.8406606372407202\n",
      "- Brier score: 0.10829268292682927\n",
      "----------------------------------\n",
      "Model performance for Test set\n",
      "- Accuracy: 0.8908382066276803\n",
      "- MCC: 0.0\n",
      "- F1 score: 0.8394083720182473\n",
      "- Brier score: 0.10829268292682927\n"
     ]
    }
   ],
   "source": [
    "# Define estimators\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "estimator_list = [\n",
    "    ('knn',knn),\n",
    "    ('svm_rbf',svm_rbf),\n",
    "    ('dt',dt),\n",
    "    ('rf',rf),\n",
    "    ('mlp',mlp), \n",
    "    ('xgbc',xgbc) ]\n",
    "\n",
    "# Build stack model\n",
    "stack_model = StackingClassifier(\n",
    "    estimators=estimator_list, final_estimator=LogisticRegression()\n",
    ")\n",
    "\n",
    "# Train stacked model\n",
    "stack_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = stack_model.predict(X_train)\n",
    "y_test_pred = stack_model.predict(X_test)\n",
    "\n",
    "# Training set model performance\n",
    "stack_model_train_accuracy = accuracy_score(y_train, y_train_pred) # Calculate Accuracy\n",
    "stack_model_train_mcc = matthews_corrcoef(y_train, y_train_pred) # Calculate MCC\n",
    "stack_model_train_f1 = f1_score(y_train, y_train_pred, average='weighted') # Calculate F1-score\n",
    "stack_model_train_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "# Test set model performance\n",
    "stack_model_test_accuracy = accuracy_score(y_test, y_test_pred) # Calculate Accuracy\n",
    "stack_model_test_mcc = matthews_corrcoef(y_test, y_test_pred) # Calculate MCC\n",
    "stack_model_test_f1 = f1_score(y_test, y_test_pred, average='weighted') # Calculate F1-score\n",
    "stack_model_test_brier = brier_score_loss(y_train, y_train_pred) # Calculate Brier Score\n",
    "\n",
    "print('Model performance for Training set')\n",
    "print('- Accuracy: %s' % stack_model_train_accuracy)\n",
    "print('- MCC: %s' % stack_model_train_mcc)\n",
    "print('- F1 score: %s' % stack_model_train_f1)\n",
    "print('- Brier score: %s' % stack_model_train_brier)\n",
    "print('----------------------------------')\n",
    "print('Model performance for Test set')\n",
    "print('- Accuracy: %s' % stack_model_test_accuracy)\n",
    "print('- MCC: %s' % stack_model_test_mcc)\n",
    "print('- F1 score: %s' % stack_model_test_f1)\n",
    "print('- Brier score: %s' % stack_model_test_brier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_train_list = {'knn':knn_train_accuracy,\n",
    "'svm_rbf': svm_rbf_train_accuracy,\n",
    "'dt': dt_train_accuracy,\n",
    "'rf': rf_train_accuracy,\n",
    "'mlp': mlp_train_accuracy,\n",
    "'xgbc': xgbc_train_accuracy,\n",
    "'stack': stack_model_train_accuracy}\n",
    "\n",
    "mcc_train_list = {'knn':knn_train_mcc,\n",
    "'svm_rbf': svm_rbf_train_mcc,\n",
    "'dt': dt_train_mcc,\n",
    "'rf': rf_train_mcc,\n",
    "'mlp': mlp_train_mcc,\n",
    "'xgbc': xgbc_train_mcc,\n",
    "'stack': stack_model_train_mcc}\n",
    "\n",
    "f1_train_list = {'knn':knn_train_f1,\n",
    "'svm_rbf': svm_rbf_train_f1,\n",
    "'dt': dt_train_f1,\n",
    "'rf': rf_train_f1,\n",
    "'mlp': mlp_train_f1,\n",
    "'xgbc': xgbc_train_f1,\n",
    "'stack': stack_model_train_f1}\n",
    "\n",
    "brier_train_list = {'knn':knn_train_brier,\n",
    "'svm_rbf': svm_rbf_train_brier,\n",
    "'dt': dt_train_brier,\n",
    "'rf': rf_train_brier,\n",
    "'mlp': mlp_train_brier,\n",
    "'xgbc': xgbc_train_brier,\n",
    "'stack': stack_model_train_brier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1</th>\n",
       "      <th>Brier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.907317</td>\n",
       "      <td>0.375312</td>\n",
       "      <td>0.885044</td>\n",
       "      <td>0.092683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_rbf</th>\n",
       "      <td>0.998049</td>\n",
       "      <td>0.989868</td>\n",
       "      <td>0.998041</td>\n",
       "      <td>0.001951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.275382</td>\n",
       "      <td>0.867948</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.982927</td>\n",
       "      <td>0.909130</td>\n",
       "      <td>0.982277</td>\n",
       "      <td>0.017073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>0.891707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840661</td>\n",
       "      <td>0.108293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgbc</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack</th>\n",
       "      <td>0.891707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.840661</td>\n",
       "      <td>0.108293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Accuracy       MCC        F1     Brier\n",
       "knn      0.907317  0.375312  0.885044  0.092683\n",
       "svm_rbf  0.998049  0.989868  0.998041  0.001951\n",
       "dt       0.900000  0.275382  0.867948  0.100000\n",
       "rf       0.982927  0.909130  0.982277  0.017073\n",
       "mlp      0.891707  0.000000  0.840661  0.108293\n",
       "xgbc     1.000000  1.000000  1.000000  0.000000\n",
       "stack    0.891707  0.000000  0.840661  0.108293"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = pd.DataFrame.from_dict(acc_train_list, orient='index', columns=['Accuracy'])\n",
    "mcc_df = pd.DataFrame.from_dict(mcc_train_list, orient='index', columns=['MCC'])\n",
    "f1_df = pd.DataFrame.from_dict(f1_train_list, orient='index', columns=['F1'])\n",
    "brier_df = pd.DataFrame.from_dict(brier_train_list, orient='index', columns=['Brier'])\n",
    "df = pd.concat([acc_df, mcc_df, f1_df, brier_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc_test_list = {'knn':knn_test_accuracy,\n",
    "'svm_rbf': svm_rbf_test_accuracy,\n",
    "'dt': dt_test_accuracy,\n",
    "'rf': rf_test_accuracy,\n",
    "'mlp': mlp_test_accuracy,\n",
    "'xgbc': xgbc_test_accuracy,\n",
    "'stack': stack_model_test_accuracy}\n",
    "\n",
    "mcc_test_list = {'knn':knn_test_mcc,\n",
    "'svm_rbf': svm_rbf_test_mcc,\n",
    "'dt': dt_test_mcc,\n",
    "'rf': rf_test_mcc,\n",
    "'mlp': mlp_test_mcc,\n",
    "'xgbc': xgbc_test_mcc,\n",
    "'stack': stack_model_test_mcc}\n",
    "\n",
    "f1_test_list = {'knn':knn_test_f1,\n",
    "'svm_rbf': svm_rbf_test_f1,\n",
    "'dt': dt_test_f1,\n",
    "'rf': rf_test_f1,\n",
    "'mlp': mlp_test_f1,\n",
    "'xgbc': xgbc_test_f1,\n",
    "'stack': stack_model_test_f1}\n",
    "\n",
    "brier_test_list = {'knn':knn_test_brier,\n",
    "'svm_rbf': svm_rbf_test_brier,\n",
    "'dt': dt_test_brier,\n",
    "'rf': rf_test_brier,\n",
    "'mlp': mlp_test_brier,\n",
    "'xgbc': xgbc_test_brier,\n",
    "'stack': stack_model_test_brier}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>MCC</th>\n",
       "      <th>F1</th>\n",
       "      <th>Brier</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>knn</th>\n",
       "      <td>0.873294</td>\n",
       "      <td>0.074882</td>\n",
       "      <td>0.842041</td>\n",
       "      <td>0.126706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>svm_rbf</th>\n",
       "      <td>0.890838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839408</td>\n",
       "      <td>0.109162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dt</th>\n",
       "      <td>0.877193</td>\n",
       "      <td>0.000835</td>\n",
       "      <td>0.835797</td>\n",
       "      <td>0.122807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.886940</td>\n",
       "      <td>-0.021900</td>\n",
       "      <td>0.837462</td>\n",
       "      <td>0.113060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mlp</th>\n",
       "      <td>0.890838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839408</td>\n",
       "      <td>0.109162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgbc</th>\n",
       "      <td>0.883041</td>\n",
       "      <td>0.056844</td>\n",
       "      <td>0.842099</td>\n",
       "      <td>0.116959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stack</th>\n",
       "      <td>0.890838</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.839408</td>\n",
       "      <td>0.108293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Accuracy       MCC        F1     Brier\n",
       "knn      0.873294  0.074882  0.842041  0.126706\n",
       "svm_rbf  0.890838  0.000000  0.839408  0.109162\n",
       "dt       0.877193  0.000835  0.835797  0.122807\n",
       "rf       0.886940 -0.021900  0.837462  0.113060\n",
       "mlp      0.890838  0.000000  0.839408  0.109162\n",
       "xgbc     0.883041  0.056844  0.842099  0.116959\n",
       "stack    0.890838  0.000000  0.839408  0.108293"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc_df = pd.DataFrame.from_dict(acc_test_list, orient='index', columns=['Accuracy'])\n",
    "mcc_df = pd.DataFrame.from_dict(mcc_test_list, orient='index', columns=['MCC'])\n",
    "f1_df = pd.DataFrame.from_dict(f1_test_list, orient='index', columns=['F1'])\n",
    "brier_df = pd.DataFrame.from_dict(brier_test_list, orient='index', columns=['Brier'])\n",
    "df = pd.concat([acc_df, mcc_df, f1_df, brier_df], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../models/list_of_features.joblib']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Guardar el modelo en un archivo\n",
    "joblib.dump(svm_rbf, '../models/svm_model.joblib')\n",
    "joblib.dump(xgbc, '../models/xgbc_model.joblib')\n",
    "joblib.dump(stack_model, '../models/stack_model.joblib')\n",
    "\n",
    "# Guardar listado de features para el modelo\n",
    "joblib.dump(selected_columns, '../models/list_of_features.joblib')\n",
    "\n",
    "# Cargar el modelo desde el archivo\n",
    "# loaded_model = joblib.load('stack_model.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
